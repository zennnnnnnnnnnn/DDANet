## DDA: A dual-domain attention plug-and-play prior for pansharpening
**homepage**

https://liangjiandeng.github.io/

https://zhi-xuan-chen.github.io/

- Code for paper: " DDA: A dual-domain attention plug-and-play prior for pansharpening"

## Citation

```
@article{DDA,
author = {Wen-Jie Shu and Zi-En Zhang},
title = {DDA: A dual-domain attention plug-and-play prior for pansharpening},
conference = {iclr tiny paper},
volume = {},
pages = {},
year = {2024},
}
```

## Dependencies and Installation
Python 3.10 (Recommend to use Anaconda)

Pytorch 1.10.0

NVIDIA GPU + CUDA

Python packages: pip install numpy scipy h5py

TensorBoard

## Dataset Preparation

The datasets used in this paper is WorldView-3 (can be downloaded here), QuickBird (can be downloaded here). Due to the copyright of dataset, we can not upload the datasets, you may download the data and simulate them according to the paper.

## Get Started
Training and testing codes are in 'codes/'. Pretrained model on WorldView-3 can be found in 'codes/Weights/'. All codes will be presented after the paper is completed published. Please refer to codes/how-to-run.md for detail description.